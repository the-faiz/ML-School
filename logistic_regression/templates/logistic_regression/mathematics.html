{% extends '../base_template/base_ML_course.html' %}
{% csrf_token %}
{% block myTitle %}    Mathematics of logistic Regression {% endblock myTitle %}
{% block mainPage %}
<div class="card-body">
    <h2 class="card-title my-3">Mathematical Overview of Logistic Regression </h2>

    <article>
        <div class="card bg-success text-white my-4">
            <div class="card-title">
                <h6>
                    What will you learn in this section
                </h6>
                <ul>
                    <li> Logistic Regression Model</li>
                    <li> Cost function used in Logistic Regression</li>
                    <li> Gradient descent Optmizer for Logistic Regression</li>
                </ul>
            </div>
        </div>

        <div>
            <p>
                We will systematically explore the various components of the Logistic Regression algorithm, then we will combine all the
                components to understand the complete algorithm. Following are the major components of the Logistic Regression Model.
                <ol>
                    <li>Model</li>
                    <li>Cost Function</li>
                    <li>Optimizer</li>
                </ol>
            </p>
        </div>
        <div>
            <h5 class="text-secondary my-3">1. Model</h5>
            <p>
                Model tells us the relationship between dependent variable \(y\) and independent variables \(x\). 
                Every Algorithm has its own model equation.  Logistic regression directly models the probability of data points 
                belonging to the positive class. Logistic Regression Model calculates these probabilities in two steps <br>

                1. First It calculates logit using equation 1 <br>
                2. Secondly it calculates probability by inputting logit values into sigmoid function. <br>
                <span style="font-family: MathJax_Main;"> 
                    \begin{align}
                    h(\theta,X) =\theta_0+\theta_1*x_1+\theta_2*x_2+..\theta_k*x_k...(1) \\
                    \\ 
                    P(y=1\mid X,\theta) =\text{sigmoid}(h(\theta,X))...(2)  
                    \end{align}
                </span>
                \( \theta_0,\theta_1,...\theta_k \) are learnable parameters. Term at the right side of the equation 1 is called <strong>logit</strong>. [can you drive the logit values in terms of probability?]
                 
                Equation 2 represents probability of data point belonging to class 1 given \(X\) (input data point) and \(\theta\) learned parameters.
                 
                If we combine above two equation, final model equation for the logistic regression model is given below <br>
                <span style="font-family: MathJax_Main;">
                        \begin{align}
                        \hat{y} =\frac{1}{1+e^{-(\theta_0+\theta_1*x_1....\theta_k*x_k)}}....(3)
                        \end{align}
                </span>

                <br>

                \( \hat{y} \) is the predicted probability score. This score indicates what is the probability that the 
                given data point belongs to class 1. Once we get this score we can classify each data point to positive or 
                negative class depending upon if the score is greater than certain threshold.  For example if predicted probability 
                score is greater than or equal to 0.5 then predicted class = 1 otherwise predicted class = 0.

                
            </p>
        </div>

        <div>
            <h5 class="text-secondary my-3"> 2. Cost Function</h5>
            <p>
                Cost Function tells us the closeness of actual and predicted values of the dependent variable. Model equation 
                gives us predicted values of the dependent variable \( (\hat{y}) \) while we have the actual value of the dependent 
                variable from the data set. Logistic Regression uses binary cross entropy loss function. Below is the mathematical equation 
                of the cost function.
                <span style="font-family: MathJax_Main;">
                        \begin{align}
                        J(\theta_0,\theta_1,...\theta_k)=\frac{1}{m}* - \{ \sum_{i=1}^my_i*\log(\hat{y_i}) + \sum_{i=1}^m(1-y_i)*\log(1-\hat{y_i}) \}
                        \end{align}
                </span>
                This cost function calculates the average cost over the entire dataset. \( m \) denotes number of data point in the dataset.
                <br>
                Core Idea behind cost function is if predicted value and actual value 
                are close then cost/error should be low. If predicted value and actual value are quite different then cost/error value 
                should be high. Now let’s check if this idea holds true with the equation which we have seen above. Consider following scenarios
                <ol>
                    <li>
                        <strong>Actual class = 1 and predicted probability  = 0.9 </strong>
                        <span style="font-family: MathJax_Main;">
                            \begin{align}
                            \text{error} &= - \{ 1 * \log(0.9) + (1-1)*\log(1-0.9) \}  \\
                            &= 0.105
                            \end{align}
                            In this case the predicted probability is high and actual class = 1, we are getting a low error value.

                        </span>
                    </li>
                    <li>
                        <strong> Actual class = 1 and predicted probability = 0.1</strong>
                        <span style="font-family: MathJax_Main;">
                            \begin{align}
                            \text{error} &= - \{ 1 * \log(0.1) + (1-1)* \log(1-0.1) \}  \\
                            &= 2.302
                            \end{align}
                            In this case the predicted probability is low and actual class = 1, we are getting relatively high error.

                        </span>
                    </li>
                    <li>
                        <strong>Actual class = 0 and predicted probability = 0.1</strong>
                        <span style="font-family: MathJax_Main;">
                            \begin{align}
                            \text{error} &= - \{ 0 * \log(0.1) + (1-0)* \log(1-0.1) \}  \\
                            &= 0.105
                            \end{align}
                            In this case the predicted probability is low and actual class = 0, we are getting relatively low error.

                        </span>
                    </li> 
                    <li>
                        <strong> Actual class = 0 and predicted probability = 0.9 </strong>
                        <span style="font-family: MathJax_Main;">
                            \begin{align}
                            \text{error} &= - \{ 0 * \log(0.1) + (1-0)* \log(1-0.9) \}  \\
                            &= 2.302
                            \end{align}
                            In this case the predicted probability is high and actual class = 0, we are getting relatively high error.

                        </span>
                    </li>
                </ol>
                Above examples show whenever predicted probability is low and actual class = 1 or predicted probability 
                is high for actual class = 0 then cost value is higher. Which is desired behaviour out of a cost function. 
                These examples established that binary cross entropy can be used as a cost function.

            </p>
            
        </div>
        <div>
            <h5 class="text-secondary my-3"> 3. Optimizer </h5>
            <p>
                Optimizer helps us to get the optimum value of parameters \( \theta_0, \theta_1.. \theta_k \) such that average cost 
                over the entire dataset is minimised.   Now we will discuss the gradient descent optimizer in the context of logistic 
                regression. We will discuss how Optimizer combines model equation, cost function, data and iteratively calculates
                 values of model parameters such that average cost over the entire dataset is minimised. <br>

                We have already seen Gradient descent in the Linear Regression Chapter. For detailed understanding of gradient 
                descent refer to this section.
                
                Gradient descent has three major steps. 

                <ol>
                    <li>
                      <strong> Parameter Initialization </strong>
                      <p>
                        First of all it will initialise all the model parameters randomly. We can initialise model parameters with any value. 
                        For simplicity let’s initialise them with 0 .
                        <br>
                        <span style="font-family: MathJax_Main;">\(  \theta_0 \ = \ 0, \  \theta_1 \ = \ 0 \ . . . \ \theta_k \ = \ 0 \)  </span>
                        
                      </p>
                    </li>
                    <li>
                        <strong>Gradient Calculation  </strong>

                        <p>
                            At this step we calculate the gradient of cost function w.r.t. Model parameters.
                            We can use following equation to calculate the gradients. <br>
                            <span style="font-family: MathJax_Main;">
                                \begin{align}
                                \frac{\partial J}{\partial \theta_0}&=\frac{1}{m}*\sum_{i=1}^m(\hat{y_i}-y_i) \\ 
                                \frac{\partial J}{\partial \theta_1}&=\frac{1}{m}*\sum_{i=1}^m(\hat{y_i}-y_i) * X_1 
                                \\ .\\.\\ 
                                \frac{\partial J}{\partial \theta_k}&=\frac{1}{m}*\sum_{i=1}^m(\hat{y_i}-y_i)*X_k
                                \end{align}

                            </span>
                            Detailed derivation of these equations can be found <a href="#">here.</a>

                        </p>
                    </li>
                    <li>
                        <strong> Update Model Parameters </strong>

                        <p>
                            Final step is to perform an update of model parameters using the calculated gradients. 
                            Gradient descent uses the below equations to perform the update step.
                            <span style="font-family: MathJax_Main;">
                                \begin{align}
                                    \theta_0&=\theta_0 - \alpha*\partial \theta_0 
                                    \\
                                    \theta_1&=\theta_1 - \alpha*\partial \theta_1
                                     \\ . \\ . \\
                                    
                                    \theta_k&=\theta_k - \alpha*\partial \theta_k
                                    \end{align}
                            
                            \( \theta_0, \theta_1 ... \theta_k \) represents model parameters <br>
                            \( \alpha \) represents learning rate <br>
                            \( \partial \theta_0, \partial \theta_1 … \partial \theta_k \) represent gradients of cost function.

                        </span>
                        </p>
                    </li>
                </ol>
                Gradient descent will keep repeating steps 2 and 3 until a stopping criteria is reached. In each iteration
                 gradient descent tends to move towards the minimum point and learning rate controls the step size in update step. 

            </p>
        </div>
        
  
    </article>
</div>
{% endblock mainPage %}

{% block rightSideCard %}
<!-- right side card contains list of definitions-->
<ul>
    <li>
        <a href="#">Sigmoid function</a>
    </li>
    <li>
        <a href="#">Case Study</a>
    </li>
</ul>
{% endblock rightSideCard %}